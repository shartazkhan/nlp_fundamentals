{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sGOlitBOZI9z",
        "GJArGmd0b9VA",
        "dcd0faba",
        "BdSN_uoTcAor",
        "a965bb2b",
        "Mc3cBq5_cHGt"
      ],
      "authorship_tag": "ABX9TyPx8usaS4XkU//QNsRQFEBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shartazkhan/nlp_fundamentals/blob/main/NLP_Text_Representation%20(Machine_Learning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12441dc"
      },
      "source": [
        "## Text Representation / Vectorization / Feature Extraction /   in NLP\n",
        "\n",
        "In Natural Language Processing (NLP), **text representation** refers to the techniques used to convert human language (text) into a format that computers can understand and process. Since computers work with numbers, text representation methods transform words, sentences, or documents into numerical vectors or matrices.\n",
        "\n",
        "Key goals of text representation include:\n",
        "\n",
        "*   **Capturing meaning:** Representing the semantic relationships between words.\n",
        "*   **Dimensionality reduction:** Reducing the complexity of the data.\n",
        "*   **Enabling analysis:** Making text data suitable for machine learning algorithms.\n",
        "\n",
        "Common techniques include:\n",
        "\n",
        "*   **Bag-of-Words (BoW):** Represents text as a collection of word counts, ignoring word order.\n",
        "*   **TF-IDF (Term Frequency-Inverse Document Frequency):** Weighs words based on their frequency in a document and rarity across a corpus.\n",
        "*   **Word Embeddings (e.g., Word2Vec, GloVe):** Dense vector representations where words with similar meanings are closer in vector space.\n",
        "*   **Sentence and Document Embeddings (e.g., BERT, transformers):** More advanced methods that capture contextual information and represent longer pieces of text.\n",
        "\n",
        "Choosing the right representation method depends on the specific NLP task and the characteristics of the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Things we will try today**\n",
        "\n",
        "* One-Hot Encoding (OHE)\n",
        "* Bag-of-Words (BoW)\n",
        "* N-grams\n",
        "* Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "* Custom Features\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k-9NKxMeVYWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot Encoding (OHE)"
      ],
      "metadata": {
        "id": "sGOlitBOZI9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Before you start, Learn about **Key Terms in NLP**\n",
        "\n",
        "*   **Corpus (C):** A large and structured collection of texts. It can be a collection of documents, books, articles, or any body of written or spoken language used for linguistic analysis and model training.\n",
        "*   **Vocabulary (V):** The set of all unique words that appear in a corpus. It's the complete list of distinct tokens that the NLP model or technique will work with.\n",
        "*   **Document (D):** A single text unit within a corpus. This could be a sentence, a paragraph, an article, a book, or any other defined piece of text.\n",
        "*   **Word (W):** The basic unit of text. In NLP, words are often referred to as tokens, especially after the text has been processed (e.g., lowercased, punctuation removed).\n",
        "\n"
      ],
      "metadata": {
        "id": "9lvIyer7WtsQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882d0b20"
      },
      "source": [
        "* * *\n",
        "\n",
        "## Advantages and Disadvantages of One-Hot Encoding (OHE) in NLP\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* **Simple to Understand and Implement:** OHE is conceptually straightforward and easy to implement.\n",
        "* **Preserves Uniqueness:** Each unique word is assigned a unique vector, ensuring that distinct words are represented differently.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* **High Dimensionality:** For a large vocabulary, the resulting vectors are very sparse and high-dimensional, leading to increased memory usage and computational complexity.\n",
        "* **No Semantic Relationship Captured:** OHE treats each word as independent and does not capture any semantic similarity or relationship between words (e.g., \"king\" and \"queen\" would have completely different vectors).\n",
        "* **Out-of-Vocabulary (OOV) Words:** OHE cannot handle words that were not present in the vocabulary during training.\n",
        "* **No Fixed Size:** The vector size depends on the vocabulary size, which can grow very large and is not fixed, making it difficult to use in models that require fixed-size input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e35016a"
      },
      "source": [
        "## One-Hot Encoding (OHE)\n",
        "\n",
        "Imagine you have a list of unique words from some text. One-Hot Encoding is like giving each unique word its own special box.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1.  **Create a list of all unique words (Vocabulary):** Go through all your text and make a list of every word that appears, but only include each word once. This is your vocabulary.\n",
        "\n",
        "2.  **Create a box for each word:** For every word in your vocabulary, create a box (or a space in a line of numbers).\n",
        "\n",
        "3.  **Put a \"1\" in the word's box and \"0\" in others:** To represent a specific word, you go to its special box and put a \"1\" in it. All the other boxes for the other words get a \"0\".\n",
        "\n",
        "Think of it like a checklist where you check off the word you are looking at.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say our unique words (vocabulary) are: \"cat\", \"dog\", \"fish\".\n",
        "\n",
        "*   To represent \"cat\", we'd have: \\[1, 0, 0] (1 for cat, 0 for dog, 0 for fish)\n",
        "*   To represent \"dog\", we'd have: \\[0, 1, 0] (0 for cat, 1 for dog, 0 for fish)\n",
        "*   To represent \"fish\", we'd have: \\[0, 0, 1] (0 for cat, 0 for dog, 1 for fish)\n",
        "\n",
        "Each word gets a unique \"hot\" spot (the 1) in a long list of zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words (BoW)\n"
      ],
      "metadata": {
        "id": "GJArGmd0b9VA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd0faba"
      },
      "source": [
        "Bag-of-Words (BoW) is another way to turn text into numbers that computers can understand. Imagine you have a bag, and you put all the words from a piece of text into that bag. The **Bag-of-Words** model just counts how many times each word appears in the text, without caring about the order of the words.\n",
        "\n",
        "Here's the simple idea:\n",
        "\n",
        "1.  **Create a list of all unique words (Vocabulary):** Just like with One-Hot Encoding, you start by finding all the unique words in your entire collection of texts.\n",
        "2.  **Count words in each document:** For each piece of text (document), you go through it and count how many times each word from your vocabulary appears.\n",
        "3.  **Create a vector:** You then create a list of numbers (a vector) for each document. Each number in the vector corresponds to a word in your vocabulary, and its value is the count of that word in the document.\n",
        "\n",
        "**Why use Bag-of-Words?**\n",
        "\n",
        "*   **Simplicity:** It's a very easy concept to understand and implement.\n",
        "*   **Good for basic tasks:** It can be effective for tasks where word order doesn't matter as much, like text classification (e.g., spam detection) or topic modeling.\n",
        "*   **Provides a numerical representation:** It converts text into a format that can be used by many machine learning algorithms.\n",
        "\n",
        "**When to use Bag-of-Words?**\n",
        "\n",
        "*   When you need a simple and quick way to represent text numerically.\n",
        "*   When the task doesn't heavily rely on understanding the exact sequence of words (like sentiment analysis where the presence of certain words is more important than their order).\n",
        "*   As a baseline model before trying more complex techniques.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say our vocabulary is: \\[\"cat\", \"dog\", \"the\", \"quick\", \"brown\", \"fox\"]\n",
        "\n",
        "Document 1: \"the quick brown fox\"\n",
        "BoW representation: \\[0, 0, 1, 1, 1, 1] (Counts of \"cat\", \"dog\", \"the\", \"quick\", \"brown\", \"fox\")\n",
        "\n",
        "Document 2: \"the quick brown cat\"\n",
        "BoW representation: \\[1, 0, 1, 1, 1, 0] (Counts of \"cat\", \"dog\", \"the\", \"quick\", \"brown\", \"fox\")\n",
        "\n",
        "Notice that the order of words in the original document doesn't affect the final vector. The vector only contains the counts of each word from the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18c0b750"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96kiIqc9cDZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fURm8beBcGcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azwu9LD_cIxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "metadata": {
        "id": "1CgwEn9D1wXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "        'this is the first document.',\n",
        "        'this is the second document.',\n",
        "        'this second is the   document.',\n",
        "        'document is this the first ',\n",
        "    ], 'label': [1,0,1,0]\n",
        "})"
      ],
      "metadata": {
        "id": "2ldCYjwq2aa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "qko0tnWi2LTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = count_vectorizer.fit_transform(df['text'])"
      ],
      "metadata": {
        "id": "3zgAshsg2MwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Sxnlu-3qGB",
        "outputId": "47e624c7-fcd6-4c60-e90f-2fc5510d49bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 5, 'is': 2, 'the': 4, 'first': 1, 'document': 0, 'second': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow[0].toarray())\n",
        "print(bow[1].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6hvJTWc3tos",
        "outputId": "ce0c0de1-749a-4269-e262-d8ed9dfe606e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 1 1]]\n",
            "[[1 0 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer.transform(['this second document is the first document']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h9_f6Vt4p89",
        "outputId": "e2352087-e380-424e-b592-98347bfc9c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What if we use a new word??**\n"
      ],
      "metadata": {
        "id": "d-5UzEwt6KWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer.transform(['here is the second and the first document']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8eq4SkV5Xw3",
        "outputId": "75e6a989-d534-4191-c320-b2108d7aab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see nothing happens!\n",
        "\n",
        "Out of vocabulary words will be ignored.\n",
        "\n",
        "Learn more [click here.](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ],
      "metadata": {
        "id": "l_tvVpSl6iyu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a12fc3ef"
      },
      "source": [
        "## Advantages and Disadvantages of Bag-of-Words (BoW)\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "*   **Simplicity and Ease of Implementation:** BoW is a straightforward model that is easy to understand and implement.\n",
        "*   **Effective for certain tasks:** It can be quite effective for tasks like text classification and topic modeling where the presence and frequency of words are more important than their order.\n",
        "*   **Provides a numerical representation:** It successfully converts text data into a numerical format that can be used by various machine learning algorithms.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "*   **Loss of Word Order/Context:** The most significant disadvantage is that BoW completely ignores the order and context of words, which can be crucial for understanding the meaning of a sentence (e.g., \"the dog bit the man\" and \"the man bit the dog\" would have the same BoW representation).\n",
        "*   **High Dimensionality and Sparsity:** Similar to OHE, for a large vocabulary, the resulting vectors can be very high-dimensional and sparse (mostly zeros), leading to increased memory usage and computational costs.\n",
        "*   **Out-of-Vocabulary (OOV) Words:** BoW models cannot handle words that were not present in the training vocabulary.\n",
        "*   **Doesn't Capture Semantic Meaning:** It doesn't capture the semantic relationships between words (e.g., \"king\" and \"queen\" are treated as completely unrelated)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zPtCvbeg-cJS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1G2l7q2E-OF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams\n"
      ],
      "metadata": {
        "id": "BdSN_uoTcAor"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-n30cTaA6fyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a965bb2b"
      },
      "source": [
        "N-grams are a way to represent text that considers sequences of words, not just individual words like in Bag-of-Words.\n",
        "\n",
        "Think of it like this: instead of just looking at single words in a sentence, you look at groups of 'N' words that appear right next to each other.\n",
        "\n",
        "Here's the simple idea:\n",
        "\n",
        "1.  **Choose a value for 'N':** This is how many words you want to group together.\n",
        "    *   If N=1, you get unigrams (single words) - this is like Bag-of-Words.\n",
        "    *   If N=2, you get bigrams (pairs of words).\n",
        "    *   If N=3, you get trigrams (groups of three words).\n",
        "    *   And so on...\n",
        "\n",
        "2.  **Slide a window of size 'N' across your text:** Start at the beginning of your text and take the first 'N' words. Then, move one word over and take the next 'N' words, and keep doing this until you reach the end of the text.\n",
        "\n",
        "3.  **Collect all the N-grams:** The groups of words you collected are your N-grams.\n",
        "\n",
        "**Why use N-grams?**\n",
        "\n",
        "*   **Captures some word order:** Unlike Bag-of-Words, N-grams keep some information about the sequence of words, which can be important for understanding context.\n",
        "*   **Useful for tasks like:**\n",
        "    *   **Text generation:** Predicting the next word based on the previous N-1 words.\n",
        "    *   **Spelling correction:** Identifying common sequences of characters or words.\n",
        "    *   **Language identification:** Different languages have different common N-grams.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's take the sentence: \"The quick brown fox\"\n",
        "\n",
        "*   **Unigrams (N=1):** \"The\", \"quick\", \"brown\", \"fox\"\n",
        "*   **Bigrams (N=2):** \"The quick\", \"quick brown\", \"brown fox\"\n",
        "*   **Trigrams (N=3):** \"The quick brown\", \"quick brown fox\"\n",
        "\n",
        "We can use scikit-learn's `CountVectorizer` again, but this time we'll specify the `ngram_range` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ede11b",
        "outputId": "1d3484b4-ba89-42b6-da80-2c0de3684098"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "        'this is the first document.',\n",
        "        'this is the second document.',\n",
        "        'this second is the   document.',\n",
        "        'document is this the first ',\n",
        "    ], 'label': [1,0,1,0]\n",
        "})\n",
        "\n",
        "# Using CountVectorizer to get N-grams\n",
        "# Let's try bigrams (N=2)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Fit and transform the text data\n",
        "X_ngram = ngram_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Print the vocabulary of bigrams\n",
        "print(\"Bigram Vocabulary:\", ngram_vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Vocabulary: {'this is': 9, 'is the': 2, 'the first': 7, 'first document': 1, 'the second': 8, 'second document': 4, 'this second': 10, 'second is': 5, 'the document': 6, 'document is': 0, 'is this': 3, 'this the': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the bigram representation of the first document\n",
        "print(\"\\nBigram representation of the first document:\")\n",
        "print(X_ngram[0].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPI1UU_ZrEaf",
        "outputId": "82f57caf-988e-4f20-af04-eefa6598b6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram representation of the first document:\n",
            "[[0 1 1 0 0 0 0 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try trigrams (N=3)\n",
        "ngram_vectorizer_3 = CountVectorizer(ngram_range=(3, 3))\n",
        "X_ngram_3 = ngram_vectorizer_3.fit_transform(df['text'])\n",
        "print(\"\\nTrigram Vocabulary:\", ngram_vectorizer_3.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yKEVBRorGc-",
        "outputId": "4516d8d9-a578-468e-db4e-b5cc7778bee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigram Vocabulary: {'this is the': 8, 'is the first': 2, 'the first document': 6, 'is the second': 3, 'the second document': 7, 'this second is': 9, 'second is the': 5, 'is the document': 1, 'document is this': 0, 'is this the': 4, 'this the first': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> So yeah... bag of words is just Unigrams.\n",
        "\n"
      ],
      "metadata": {
        "id": "rGkADX4vqh15"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfff285e"
      },
      "source": [
        "## Advantages and Disadvantages of N-grams\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "*   **Captures some word order and context:** Unlike Bag-of-Words, N-grams preserve some sequential information, which can be important for tasks where word order matters.\n",
        "*   **Useful for various tasks:** N-grams are effective in applications like text generation, spelling correction, and language identification.\n",
        "*   **Can capture short-range dependencies:** They can identify patterns and relationships between words that appear close together.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "*   **High Dimensionality:** As the value of 'N' increases, the number of possible N-grams grows exponentially, leading to very high-dimensional and sparse feature vectors. This can increase computational cost and memory usage.\n",
        "*   **Data Sparsity:** Many possible N-grams will not appear in the corpus, resulting in a sparse representation.\n",
        "*   **Limited long-range dependency capture:** N-grams with a fixed 'N' cannot capture dependencies between words that are far apart in the text.\n",
        "*   **Out-of-Vocabulary (OOV) N-grams:** Similar to BoW, N-gram models cannot handle N-grams that were not present in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency-Inverse Document Frequency (TF-IDF)\n"
      ],
      "metadata": {
        "id": "z0Xk5feocEBP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb5f8065"
      },
      "source": [
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used in NLP to evaluate how important a word is to a document within a collection of documents (corpus). It's not just about how often a word appears in a single document (like in Bag-of-Words), but also how unique the word is across all documents.\n",
        "\n",
        "Think of it this way:\n",
        "\n",
        "*   **Term Frequency (TF):** How often a word appears in a specific document. If a word appears many times in a document, its TF is high.\n",
        "*   **Inverse Document Frequency (IDF):** How rare a word is across the entire collection of documents. If a word appears in many documents, its IDF is low. If a word appears in only a few documents, its IDF is high.\n",
        "\n",
        "**TF-IDF combines these two:**\n",
        "\n",
        "TF-IDF score for a word in a document = TF (of the word in that document) * IDF (of the word across all documents)\n",
        "\n",
        "A high TF-IDF score means the word is frequent in the document but rare in the corpus, making it likely a key term for that specific document. Words that are very common across all documents (like \"the\", \"is\", \"a\") will have a low IDF and thus a low TF-IDF score, even if they appear frequently in a single document.\n",
        "\n",
        "**Why use TF-IDF?**\n",
        "\n",
        "*   **Highlights important words:** It helps identify words that are particularly relevant to a specific document compared to the rest of the corpus.\n",
        "*   **Reduces the impact of common words:** It gives less weight to words that appear very often across all documents, which are usually less informative.\n",
        "*   **Provides a numerical representation:** Like BoW, it converts text into a numerical format suitable for machine learning.\n",
        "\n",
        "**When to use TF-IDF?**\n",
        "\n",
        "*   **Information Retrieval:** To rank documents based on how relevant they are to a query (the query terms with high TF-IDF in a document are good indicators of relevance).\n",
        "*   **Text Summarization:** To identify the most important words in a document.\n",
        "*   **Text Classification:** As features to train classifiers, where important words can help distinguish between categories.\n",
        "*   **Topic Modeling:** To understand the key terms associated with different topics.\n",
        "\n",
        "In essence, TF-IDF helps us find the words that are uniquely characteristic of a document, making it a valuable technique for many text analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "        'this is the first document.',\n",
        "        'this is the second document.',\n",
        "        'this second is the   document.',\n",
        "        'document is this the first ',\n",
        "    ], 'label': [1,0,1,0]\n",
        "})\n",
        "\n",
        "# Using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_vectorizer.fit_transform(df['text']).toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqmOF2Mb-GYA",
        "outputId": "36d02fd1-d7ba-4a96-deba-0997fe40cdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39896105, 0.60276058, 0.39896105, 0.        , 0.39896105,\n",
              "        0.39896105],\n",
              "       [0.39896105, 0.        , 0.39896105, 0.60276058, 0.39896105,\n",
              "        0.39896105],\n",
              "       [0.39896105, 0.        , 0.39896105, 0.60276058, 0.39896105,\n",
              "        0.39896105],\n",
              "       [0.39896105, 0.60276058, 0.39896105, 0.        , 0.39896105,\n",
              "        0.39896105]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(tfidf_vectorizer.idf_)\n",
        "display(tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8y9M5YK_lk4_",
        "outputId": "e9838573-8e72-4f2d-a2d4-d787b3d1af2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([1.        , 1.51082562, 1.        , 1.51082562, 1.        ,\n",
              "       1.        ])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['document', 'first', 'is', 'second', 'the', 'this'], dtype=object)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkKHjYuGmvNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn more [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)"
      ],
      "metadata": {
        "id": "BunpqEPpmwAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Features"
      ],
      "metadata": {
        "id": "Mc3cBq5_cHGt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zXCxyjcq14b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}